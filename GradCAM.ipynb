{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from gradcam.utils import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define device (GPU or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JPEG image file\n",
    "img_path = 'tiny-imagenet-200/val/images/n01882714/val_5008.JPEG'\n",
    "img = PIL.Image.open(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation sequence\n",
    "preprocess_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224), \n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Pre-processing\n",
    "torch_img = preprocess_transform(img).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image\n",
    "plt.imshow(torch_img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "model_baseline = models.resnet18(pretrained = False)\n",
    "num_ftrs = model_baseline.fc.in_features\n",
    "model_baseline.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "model_path = 'ResNet18_baseline_model.pth'\n",
    "model_baseline.load_state_dict(torch.load(model_path, map_location = torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS regularization model\n",
    "model = models.resnet18(pretrained = True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)\n",
    "\n",
    "class SplittedResNet18(nn.Module):\n",
    "    def __init__(self, resnet18):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = resnet18.fc\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        representation = self.flatten(self.cnn(x))\n",
    "        output = self.fc(representation)\n",
    "        return representation, output\n",
    "\n",
    "model_os = SplittedResNet18(model)\n",
    "\n",
    "model_path = 'ResNet18_OS_0.01.pth'\n",
    "model_os.load_state_dict(torch.load(model_path, map_location = torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Grad-CAM function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "def gradcam_baseline(model, img_fpath):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    def __extract(grad):\n",
    "        global feature_grad\n",
    "        feature_grad = grad\n",
    "        \n",
    "    img = PIL.Image.open(img_fpath).convert('RGB')\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img = transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "        \n",
    "    # get features from the last convolutional layer\n",
    "    x = model.conv1(img)\n",
    "    x = model.bn1(x)\n",
    "    x = model.relu(x)\n",
    "    x = model.maxpool(x)\n",
    "    x = model.layer1(x)\n",
    "    x = model.layer2(x)\n",
    "    x = model.layer3(x)\n",
    "    x = model.layer4(x)\n",
    "    features = x    \n",
    "    \n",
    "    # hook for the gradients\n",
    "    def __extract_grad(grad):\n",
    "        global feature_grad\n",
    "        feature_grad = grad\n",
    "    features.register_hook(__extract_grad)\n",
    "    \n",
    "    # get the output from the whole VGG architecture\n",
    "    x = model.avgpool(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    output = model.fc(x)\n",
    "    pred = torch.argmax(output).item()\n",
    "    print(pred)    \n",
    "    \n",
    "    # get the gradient of the output\n",
    "    output[:, pred].backward()\n",
    "    \n",
    "    # pool the gradients across the channels\n",
    "    pooled_grad = torch.mean(feature_grad, dim=[0, 2, 3])\n",
    "    \n",
    "    # weight the channels with the corresponding gradients\n",
    "    # (L_Grad-CAM = alpha * A)\n",
    "    features = features.detach()\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_grad[i]\n",
    "        \n",
    "    # average the channels and create an heatmap\n",
    "    # ReLU(L_Grad-CAM)\n",
    "    heatmap = torch.mean(features, dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # normalization for plotting\n",
    "    heatmap = heatmap / torch.max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    # project heatmap onto the input image\n",
    "    img = cv2.imread(img_fpath)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap_plot = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    image_plot = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    superimposed_img = heatmap * 0.8 + img\n",
    "    superimposed_img = np.uint8(255 * superimposed_img / np.max(superimposed_img))\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (14, 3))\n",
    "    ax[0].imshow(image_plot, aspect='auto')\n",
    "    ax[1].imshow(heatmap_plot, aspect='auto')\n",
    "    ax[2].imshow(superimposed_img, aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS regularization model\n",
    "def gradcam_OS_model(model, img_fpath):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    def __extract(grad):\n",
    "        global feature_grad\n",
    "        feature_grad = grad\n",
    "        \n",
    "    img = PIL.Image.open(img_fpath).convert('RGB')\n",
    "    transforms = T.Compose([\n",
    "        T.Resize(256),\n",
    "        T.CenterCrop(224),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img = transforms(img)\n",
    "    img = img.unsqueeze(0)\n",
    "    \n",
    "    # get features from the last convolutional layer\n",
    "    x = list(model.cnn.children())[0](img)\n",
    "    x = list(model.cnn.children())[1](x)\n",
    "    x = list(model.cnn.children())[2](x)\n",
    "    x = list(model.cnn.children())[3](x)    \n",
    "    x = list(model.cnn.children())[4](x)\n",
    "    x = list(model.cnn.children())[5](x)\n",
    "    x = list(model.cnn.children())[6](x)\n",
    "    x = list(model.cnn.children())[7](x)\n",
    "    features = x  \n",
    "    \n",
    "    # hook for the gradients\n",
    "    def __extract_grad(grad):\n",
    "        global feature_grad\n",
    "        feature_grad = grad\n",
    "    features.register_hook(__extract_grad)\n",
    "    \n",
    "    # get the output from the whole VGG architecture\n",
    "    x = list(model.cnn.children())[8](x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    output = list(model.children())[2](x)\n",
    "    pred = torch.argmax(output).item()\n",
    "    print(pred)   \n",
    "\n",
    "    # get the gradient of the output\n",
    "    output[:, pred].backward()\n",
    "    \n",
    "    # pool the gradients across the channels\n",
    "    pooled_grad = torch.mean(feature_grad, dim=[0, 2, 3])\n",
    "    \n",
    "    # weight the channels with the corresponding gradients\n",
    "    # (L_Grad-CAM = alpha * A)\n",
    "    features = features.detach()\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_grad[i]\n",
    "        \n",
    "    # average the channels and create an heatmap\n",
    "    # ReLU(L_Grad-CAM)\n",
    "    heatmap = torch.mean(features, dim=1).squeeze()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # normalization for plotting\n",
    "    heatmap = heatmap / torch.max(heatmap)\n",
    "    heatmap = heatmap.numpy()\n",
    "    \n",
    "    # project heatmap onto the input image\n",
    "    img = cv2.imread(img_fpath)\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    heatmap_plot = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    image_plot = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    superimposed_img = heatmap * 0.8 + img\n",
    "    superimposed_img = np.uint8(255 * superimposed_img / np.max(superimposed_img))\n",
    "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize = (14, 3))\n",
    "    ax[0].imshow(image_plot, aspect='auto')\n",
    "    ax[1].imshow(heatmap_plot, aspect='auto')\n",
    "    ax[2].imshow(superimposed_img, aspect='auto')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Grad-CAM image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'tiny-imagenet-200/val/images/n01882714/val_5008.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_baseline(model_baseline, img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_OS_model(model_os, img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_venv",
   "language": "python",
   "name": "pytorch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
